# ğŸ§  Convolutional Neural Network â€” From Scratch & PyTorch

This repository demonstrates two complete implementations of a Convolutional Neural Network (CNN):

CNN from Scratch (NumPy only) â€” manual forward & backward propagation

CNN using PyTorch â€” framework-based, production-style implementation

The goal is deep conceptual understanding + practical engineering skills.

Most projects show how to use CNNs.
This project shows how CNNs actually work internally.

# ğŸ” Why This Project Matters

Builds CNNs mathematically from first principles

Implements manual backpropagation through convolution

Demonstrates ability to translate theory â†’ code

Shows framework independence (NumPy â†’ PyTorch)

Strong signal for ML Intern / SWE / Research roles

This repository is designed for:

ML / DL interview preparation

Research-oriented learning

Systems-level understanding of deep learning

# ğŸ—‚ï¸ Repository Structure
CNN-From-Scratch-Project/
â”‚
â”œâ”€â”€ scratch_cnn/                 # NumPy-only implementation
â”‚   â”œâ”€â”€ conv_forward.py
â”‚   â”œâ”€â”€ conv_backward.py
â”‚   â”œâ”€â”€ im2col.py
â”‚   â”œâ”€â”€ col2im.py
â”‚   â”œâ”€â”€ relu.py
â”‚   â”œâ”€â”€ maxpool.py
â”‚   â”œâ”€â”€ flatten.py
â”‚   â”œâ”€â”€ dense.py
â”‚   â”œâ”€â”€ softmax_loss.py
â”‚   â””â”€â”€ cnn_model.py
â”‚
â”œâ”€â”€ pytorch_cnn/                 # PyTorch implementation
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ dataset.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ CNN_From_Scratch.ipynb
â”‚   â””â”€â”€ CNN_PyTorch.ipynb
â”‚
â”œâ”€â”€ visuals/
â”‚   â”œâ”€â”€ training_loss_curve.png
â”‚   â”œâ”€â”€ feature_maps.png
â”‚   â””â”€â”€ learned_filters.png
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

# ğŸ§  CNN Architecture (Both Versions)
Input Image (N, 1, 28, 28)
â†“
Convolution (3Ã—3 filters)
â†“
ReLU
â†“
Max Pooling (2Ã—2)
â†“
Flatten
â†“
Fully Connected Layer
â†“
Softmax
â†“
Cross-Entropy Loss

# ğŸ§ª Implementation 1: CNN From Scratch (NumPy)
âœ… Whatâ€™s implemented manually

Convolution (forward & backward)

im2col / col2im optimizations

ReLU activation (forward & backward)

Max Pooling (forward & backward)

Dense layer

Softmax + Cross Entropy loss

Gradient computation using chain rule

Shape-safe tensor handling

# ğŸ§  Concepts Covered

Sliding window convolution

Receptive fields

Parameter sharing

Gradient flow through convolution

Numerical stability

Manual backpropagation

This implementation does not use TensorFlow or PyTorch â€” only NumPy.

# âš¡ Implementation 2: CNN Using PyTorch
âœ… Whatâ€™s included

Modular CNN model (nn.Module)

Clean training loop

Dataset abstraction

Loss & optimizer handling

GPU-ready architecture

Comparison with scratch implementation

# ğŸ¯ Purpose

Show production-style ML engineering

Validate scratch implementation correctness

Bridge theory â†’ real-world ML pipelines

# ğŸ“Š Visualizations

Generated through notebooks:

ğŸ“‰ Training loss curves

ğŸ§© Feature maps after convolution

ğŸ¯ Learned filters visualization

ğŸ“Š Softmax probability outputs

These help interpret what the CNN is learning internally, not just final accuracy.

â–¶ï¸ How to Run
1ï¸âƒ£ Clone the repository
git clone https://github.com/anshupatna06/CNN-From-Scratch-Project.git
cd CNN-From-Scratch-Project

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run notebooks
jupyter notebook


Open:

notebooks/CNN_From_Scratch.ipynb

notebooks/CNN_PyTorch.ipynb

Run cells top-to-bottom.

# ğŸ§ª Notes

Scratch CNN uses synthetic / small-scale data for clarity

Focus is understanding, not benchmark accuracy

Code is extensible to datasets like MNIST

# ğŸ”® Future Improvements

Train both versions on MNIST

Add Adam optimizer

Batch Normalization

Multiple convolution blocks

Unit tests for gradients

Performance comparison (NumPy vs PyTorch)

# ğŸ¯ Learning Outcomes

By completing this project, you will:

Understand CNNs mathematically and programmatically

Gain confidence in debugging deep learning models

Be able to explain CNN internals in interviews

Demonstrate framework-agnostic ML thinking


â­ If this repository helped you, feel free to star it!
