# CNN-From-Scratch-Project
# ğŸ§  Convolutional Neural Network (CNN) From Scratch â€” NumPy Only

This project implements a **full Convolutional Neural Network (CNN) from scratch** using only **NumPy**, without relying on deep learning frameworks like TensorFlow or PyTorch.

The goal of this project is to deeply understand:
- How CNNs work mathematically
- How forward and backward propagation are implemented
- How filters are learned
- How spatial dimensions flow through layers
- How gradients are computed manually

This repository is designed for **learning, research, and interview preparation**.

---

## ğŸš€ Key Features

-  Convolution forward & backward pass (from scratch)
-  `im2col` and `col2im` optimization
-  ReLU activation (forward & backward)
-  Max Pooling layer (forward & backward)
-  Flatten layer
-  Fully Connected (Dense) layer
-  Softmax + Cross Entropy loss
-  Complete CNN pipeline
-  Shape-safe implementation
-  Jupyter notebook for visualizations
-  Feature map & filter visualizations

---

## ğŸ§± Project Structure
CNN-From-Scratch-Project/
â”‚
â”œâ”€â”€ src/ # Core CNN implementation
â”‚ â”œâ”€â”€ conv_forward.py
â”‚ â”œâ”€â”€ conv_backward.py
â”‚ â”œâ”€â”€ im2col.py
â”‚ â”œâ”€â”€ col2im.py
â”‚ â”œâ”€â”€ relu.py
â”‚ â”œâ”€â”€ maxpool.py
â”‚ â”œâ”€â”€ flatten.py
â”‚ â”œâ”€â”€ dense.py
â”‚ â”œâ”€â”€ softmax_loss.py
â”‚ â””â”€â”€ cnn_model.py
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ CNN_From_Scratch.ipynb # Training & visualizations
â”‚
â”œâ”€â”€ visuals/ # Generated visual outputs
â”‚ â”œâ”€â”€ training_loss_curve.png
â”‚ â”œâ”€â”€ feature_maps.png
â”‚ â”œâ”€â”€ learned_filters.png
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt


---

## ğŸ§  CNN Architecture

Input Image (N, 1, 28, 28)
â†“
Convolution (3Ã—3 filters)
â†“
ReLU
â†“
Max Pooling (2Ã—2)
â†“
Flatten
â†“
Fully Connected Layer
â†“
Softmax
â†“
Loss

---

## ğŸ“ Mathematical Concepts Used

- Convolution operation
- Sliding window & receptive fields
- `im2col` matrix transformation
- Backpropagation through convolution
- Chain rule
- Gradient descent
- Softmax probability distribution
- Cross-entropy loss

---

## ğŸ“Š Visualizations Included

The Jupyter notebook generates:
- ğŸ“‰ Training loss curve
- ğŸ§© Feature maps after convolution
- ğŸ¯ Learned convolution filters
- ğŸ“Š Softmax output probabilities

These visualizations help in **interpreting what the CNN learns internally**.

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/anshupatna06/CNN-From-Scratch-Project.git
cd CNN-From-Scratch-Project
2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run Jupyter Notebook
jupyter notebook


Open:

notebooks/CNN_From_Scratch.ipynb


Run all cells top-to-bottom.

##ğŸ§ª Notes

The CNN is trained on dummy / synthetic data for demonstration.

The goal is conceptual clarity, not accuracy benchmarking.

The implementation is fully extensible to real datasets like MNIST.

ğŸ”® Future Improvements

Train on MNIST dataset

Add Adam optimizer

Add batch normalization

Add multiple convolution layers

Compare with PyTorch implementation

Add unit tests

ğŸ¯ Learning Outcome

By completing this project, you will:

Understand CNNs at a mathematical level

Be confident implementing deep learning models from scratch

Gain strong debugging intuition

Be well-prepared for ML/DL interviews
ğŸ™Œ Acknowledgement

Inspired by:

CS231n (Stanford)

Deep Learning Specialization

Research-oriented learning approach

â­ If you find this project helpful, feel free to star the repository!


-----------------------------------------



